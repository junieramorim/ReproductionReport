Following the overall process defined by action research, and using the new dynamic scenario (Section \ref{sec:dynamic_scenario}), the original algorithms and the extended ones (Section \ref{sec:changes}) were exercised with two replications to identify possible limitations, constraints, and opportunities for improvement. The first replication (Section \ref{sec:original}) was conducted to assess how well the original algorithms work in the dynamic scenario, whereas the second replication (Section \ref{sec:replication}) assesses the extended algorithms in the same dynamic scenario.

All replications in this work use the same time limit of the original study, i.e., a total of 300 ticks. The displacement and task execution have a time consumption and the total time consumed to perform all tasks allocated has to be within this upper limit of 300 ticks.

To perform the replications, some definitions had to be done in terms of parameters applied during the execution of the algorithms. Three independent variables changed during experiments are listed in the following. All the other parameters received the same value used by \textit{Schwarzrock et al.}~\cite{MAS07}.
\begin{itemize}
   \item \textit{stimulus}: parameter that defines a priority relation among the distance to the task and the quality of an on board sensors to perform the task. This value is considered during the agent capacity calculation (see Section~\ref{sec:background}) and the study by \textit{Ferreira et al.}~\cite{ferreira2010robocup} suggested that the best value in most situations of Swarm-GAP strategy application in static scenarios is $0.6$; However, in dynamic scenarios, there is a result impact with different values to \textit{stimulus}. Table \ref{table:stimulus} shows the intervals of difference in results using different values for the \textit{stimulus}. Since the differences are low, value of $0.6$ was fixed as standard to \textit{stimulus} for the final replications so that they have similar conditions to be compare to the results obtained by previous experiments.
   \item \textit{number of executions}: Each replication is repeated $n$ times to generate a set of results and, based on them, calculate the average and standard deviation of each attribute measured during the experiment. The number of executions was increased to 100 runs (instead of 30 used in the original study~\cite{MAS07}) for each algorithm aiming at the reduction of the resulting standard deviation, thus getting a higher precision. This increase generated results closer to the normal distribution, minimizing the standard deviation (see a sample result set with 50 executions of the extended SAL algorithm in Figure \ref{fig:fig07}).
   \item \textit{number of UAVs}: Changes in the number of active agents follow what was defined in Section \ref{sec:dynamic_scenario} and force the algorithms to reconfigure the task allocation among the UAVs. This feature makes the scenario dynamic because the number of agent changes during the experiment run time. During the allocation tasks procedure, tasks not finished by the dropped UAVs are reallocated according to the strategy explained in Section \ref{sec:changes}.
\end{itemize}

The focus of the analysis was on the broadest scenario with 9 UAVs. It was chosen because, with smaller scenarios (with lower number of UAVs), the differences in preliminary results were not statistically significant due to high standard deviation. Furthermore, the independent replication was done with the three algorithm variants (AL, SAL and LAL) presented in Section \ref{sec:background}. The source code used during the experiments and the results obtained can be accessed through a link to GitHub repository\footnote{https://github.com/junieramorim/replicationMaterial}.

\input{table04}
