Following the overall process defined by action research, and using the new dynamic scenario (see Section \ref{sec:dynamic_scenario}), the original algorithms and the extended ones (Section \ref{sec:changes}), we performed two replications to identify possible limitations, constraints, and improvement opportunities. The first replication (Section \ref{sec:original}) was conducted to assess how well the original algorithms work in the dynamic scenario, whereas the second replication (Section \ref{sec:replication}) assesses the extended algorithms in the same dynamic scenario.

Furthermore, to reduce the standard deviation in the results, all experiments were executed 100 times instead of 30 done by the original study. It was noticed an error decrease when the number of executions is greater, but more than 100 times the value reduced is despicable if compared with the 100 times deviation.

All replications done by this work are using the same limit time of the original study, in a total of 300 ticks. The displacement and task execution have a time consumption and the total has to be inside this limit.

To perform the following replications, some definitions had to be done in terms of parameters applied during algorithms executions. Three independent variables changed during experiments execution and were listed below. All other parameters received the same value used by \textit{Schwarzrock et al.}\cite{MAS07}.
\begin{itemize}
   \item \textit{stimulus}: parameter that defines a priority relation among the distance to the task and the quality of on board sensors to perform it. This value is considered during the agent capacity calculation (see Section \ref{sec:background}) and the study by \textit{Ferreira et al.} \cite{ferreira2010robocup} suggested that the best value in most situations of Swarm-GAP strategy application in static scenarios is $0.6$; However, in dynamic scenarios, there is a result impact with different values to \textit{stimulus}. Table \ref{table:stimulus} shows the intervals of difference in results using different \textit{stimulus} values. Since the differences are low, we fixed the standard value to \textit{stimulus} as $0.6$ to execute the final replications and have similar conditions to compare the results with those obtained by previous experiments.
   \item \textit{number of executions}: Each replication is repeated $n$ times to generate a set of results and, based on them, get the medium and standard deviation of each experiment measured attribute. The number of executions was increased to 100 times (instead of 30 used by the original study \cite{MAS07}) for each algorithm aiming the reduction of the results deviation, getting a higher precision. This increase generated results closer to the normal distribution, minimizing the deviation (see a sample result set with 50 executions of the extended SAL algorithm in Figure \ref{fig:fig07}).
   \item \textit{number of UAVs}: Changes in the number of active agents follow what was defined in \ref{sec:dynamic_scenario} and forces the algorithms to reconfigure the task allocation among the UAVs. This characteristic becomes the context dynamic because the agent number changes during the algorithms execution. During the allocation tasks procedure, not finished tasks of dropped UAVs are reallocated according to the strategy displayed by previous sessions.
\end{itemize}

\input{table04}
