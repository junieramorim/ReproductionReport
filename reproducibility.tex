% Esse primeiro parágrafo serve de introdução para essa subseção e referencia os estudos que serviram de base para classificar o referido experimento como reprodutível
The analysis of the empirical replications in software engineering was performed by many researchers and extensively reported in the literature~\citep{exp05, exp04, exp03, exp01}. They presented requirements to classify an experiment as reproducible and important guidelines to perform a reproduction, as well as the main experimental elements that must be observed during a replication. \textit{Madeyski and Kitchenham} \citep{exp02} presented the difference among replication and reproduction, highlighting aspects to guide an independent replication, which was the approach used in this present work.

According to \textit{Madeyski and Kitchenham}~\citep{exp02}, reproducible research (RR) is defined as a study that can be reproduced from all information and data registered about a given set of experiments. As a fundamental requirement to perform the replication, the original study was assessed for its reproducibility.

Based on that definition, the original study presented by \textit{Schwarzrock et al.}~\citep{MAS07} can be classified as a RR since all experiments previously done were reproduced, obtaining results within the standard deviation margin of the original ones\footnote{Reproduction results available at https://github.com/junieramorim/replicationMaterial}. Table \ref{table:variables} shows all independent and dependent variables used by the experiment reproduction, in which the slight differences obtained were due to the distinct hardware configurations used in the original experiments. This procedure aimed at analyzing how the results would be equal to the original experiment in similar conditions of execution, including the number of thirty runs for each algorithm.

To explore different situations regarding mission complexity and available resources, the original and the reproduced experimental scenarios were the following:

\begin{enumerate}
	\item 3 \uavs; 4 tasks; 300 ticks as deadline; 100 x 80 px area size, \label{case:4tasks}
	\item 3 \uavs; 8 tasks; 300 ticks as deadline; 100 x 80 px area size, \label{case:8tasks}
	\item 3 \uavs; 16 tasks; 300 ticks as deadline; 100 x 80 px area size, \label{case:16tasks}
	\item 3 \uavs; 32 tasks; 300 ticks as deadline; 100 x 80 px area size, \label{case:32tasks}
	\item 6 \uavs; 64 tasks; 300 ticks as deadline; 200 x 160 px area size, \label{case:64tasks}
	\item 9 \uavs; 96 tasks; 300 ticks as deadline; 300 x 240 px area size. \label{case:96tasks}
\end{enumerate}

The platform used to perform all experiments presented by this work was the same of the original study~\citep{MAS07}, NetLogo, version 5.3.1, a multi-agent programmable modeling environment. This choice ensured to keep the previous code compatibility and allowed its complete reuse, besides allowing to implement changes in reproductions, which will be described later in this text. The original code was imported to the platform, the executions were done to generate results in text format through the built-in console. With a selection and copy feature, these results were exported to a file to be read by R Studio import tool, generating final graphs.  

\input{table05}

Since the study by \textit{Schwarzrock et al.}\citep{MAS07} is reproducible, it is possible to perform independent replications from it. Replications allow exploring different contexts from the original study  by changing variables, conditions, and controls to analyze the effects on the results~\citep{exp03}. 
